{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/GIT_SHENANIGANS/oat-2025/.venv2025/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:33<00:00, 23.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Dict, Any, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from oat_evaluation.llms.llm import LLM, ExposedActivationsRequest, LLMResponses, TokenSelectionMethod\n",
    "from contextlib import contextmanager\n",
    "\n",
    "model_path = \"/workspace/gemma_2_9b_instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    # float16 was the default for obfuscated-activations too\n",
    "    model_path, device_map=\"cuda\", torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding_layer = model.get_input_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE PROMPT\n",
      "{'input_ids': tensor([[  2299,    577,  44528, 235336]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:0')}\n",
      "torch.Size([1, 4])\n",
      "tensor([[[-0.0134, -0.0056, -0.0208,  ...,  0.0049, -0.0243, -0.0239],\n",
      "         [-0.0240,  0.0229, -0.0211,  ..., -0.0066, -0.0115, -0.0057],\n",
      "         [-0.0272,  0.0236, -0.0591,  ..., -0.0204, -0.0371, -0.0564],\n",
      "         [ 0.0055, -0.0525, -0.0269,  ..., -0.0056, -0.0532, -0.0179]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 4, 3584])\n",
      "\n",
      "CHAT PROMPT\n",
      "{'input_ids': tensor([[     2,    106,   1645,    108,   2299,    577,  44528, 235336,    107,\n",
      "            108,    106,   2516,    108]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "tensor([[[-0.0112,  0.0026,  0.0081,  ..., -0.0006,  0.0043,  0.0033],\n",
      "         [-0.0444, -0.0019, -0.0378,  ..., -0.0056, -0.0254, -0.0242],\n",
      "         [-0.0223, -0.0214, -0.0588,  ..., -0.0151, -0.0009, -0.0240],\n",
      "         ...,\n",
      "         [-0.0444, -0.0019, -0.0378,  ..., -0.0056, -0.0254, -0.0242],\n",
      "         [-0.0058,  0.0115,  0.0273,  ..., -0.0038, -0.0253, -0.0239],\n",
      "         [-0.0139,  0.0752,  0.0123,  ...,  0.0013, -0.0026, -0.0114]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 13, 3584])\n",
      "Found index 4!\n",
      "SECTION SIZES:\n",
      "torch.Size([1, 4, 3584])\n",
      "torch.Size([1, 5, 3584])\n",
      "COMBINED SIZE:\n",
      "torch.Size([1, 13, 3584])\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Now.... Let's try different ways of:\n",
    "# Turning embed into chat-template embed\n",
    "# Feeding embeds into the model\n",
    "\n",
    "# First, let's establish what \"chat-template\" embed we actually want.\n",
    "# i.e. let's embed pre and post chat and compare them...\n",
    "# To identify each tensor, we'll want to hash it ig\n",
    "\n",
    "prompt = \"How to bake?\"\n",
    "\n",
    "print(\"BASE PROMPT\")\n",
    "token_ids1 = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
    "print(token_ids1)\n",
    "print(token_ids1[\"input_ids\"].shape)\n",
    "embeddings1 = model_embedding_layer(token_ids1[\"input_ids\"])\n",
    "print(embeddings1)\n",
    "print(embeddings1.shape)\n",
    "\n",
    "print(\"\\nCHAT PROMPT\")\n",
    "\n",
    "messages = [\n",
    "    [{\"role\": \"user\", \"content\": prompt}]\n",
    "]\n",
    "token_ids2 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(\"cuda\")\n",
    "print(token_ids2)\n",
    "embeddings2 = model_embedding_layer(token_ids2[\"input_ids\"])\n",
    "print(embeddings2)\n",
    "print(embeddings2.shape)\n",
    "\n",
    "#print(embeddings2[0, 0].equal(embeddings1[0, 0]))\n",
    "\n",
    "insertion_index = -1\n",
    "for i in range(embeddings2.shape[1]):\n",
    "    if embeddings2[0, i].equal(embeddings1[0, 0]):\n",
    "        # Insertion here!\n",
    "        print(f\"Found index {i}!\")\n",
    "        insertion_index = i\n",
    "        break\n",
    "\n",
    "embeddings2_intro = embeddings2[0, :insertion_index].unsqueeze(0)\n",
    "embeddings2_outro = embeddings2[0, insertion_index+embeddings1.shape[1]:].unsqueeze(0)\n",
    "\n",
    "print(f\"SECTION SIZES:\")\n",
    "print(embeddings2_intro.shape)\n",
    "print(embeddings2_outro.shape)\n",
    "\n",
    "embedding_chat_function = lambda raw: torch.cat((embeddings2_intro, raw, embeddings2_outro), dim=1)\n",
    "\n",
    "combo_embedding = embedding_chat_function(embeddings1)\n",
    "print(\"COMBINED SIZE:\")\n",
    "print(combo_embedding.shape)\n",
    "\n",
    "for i in range(embeddings2.shape[1]):\n",
    "    print(embeddings2[0, i].equal(combo_embedding[0, i]))\n",
    "\n",
    "# for each item in new embedding...\n",
    "# if it equals index in base: mark \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3584])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['input_embeds'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m pad_embedding \u001b[38;5;241m=\u001b[39m token_ids_to_embeddings(token_id_tensor)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(pad_embedding\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 12\u001b[0m generated_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombo_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/GIT_SHENANIGANS/oat-2025/.venv2025/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/GIT_SHENANIGANS/oat-2025/.venv2025/lib/python3.10/site-packages/transformers/generation/utils.py:2225\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2220\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[1;32m   2222\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(\n\u001b[1;32m   2223\u001b[0m     generation_config, use_model_defaults, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   2224\u001b[0m )\n\u001b[0;32m-> 2225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[1;32m   2228\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/GIT_SHENANIGANS/oat-2025/.venv2025/lib/python3.10/site-packages/transformers/generation/utils.py:1536\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1533\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1537\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1539\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['input_embeds'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "model_embedding_layer = model.get_input_embeddings()\n",
    "\n",
    "def token_ids_to_embeddings(token_ids):\n",
    "    \"\"\"Expects input shape (batch_size, seq_len). Outputs shape (batch_size, seq_len, embedding_size).\"\"\"\n",
    "    return model_embedding_layer(token_ids)\n",
    "\n",
    "\n",
    "token_id_tensor = torch.tensor(tokenizer.pad_token_id, device='cuda').unsqueeze(0).unsqueeze(0)\n",
    "pad_embedding = token_ids_to_embeddings(token_id_tensor)\n",
    "print(pad_embedding.shape)\n",
    "\n",
    "generated_outputs = model.generate(\n",
    "        input_embeds=combo_embedding,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to replace emebds shape torch.Size([1, 13, 3584]) with torch.Size([1, 13, 3584])\n",
      "Specifically: tensor([[[-0.0112,  0.0026,  0.0081,  ..., -0.0006,  0.0043,  0.0033],\n",
      "         [-0.0444, -0.0019, -0.0378,  ..., -0.0056, -0.0254, -0.0242],\n",
      "         [-0.0223, -0.0214, -0.0588,  ..., -0.0151, -0.0009, -0.0240],\n",
      "         ...,\n",
      "         [-0.0444, -0.0019, -0.0378,  ..., -0.0056, -0.0254, -0.0242],\n",
      "         [-0.0058,  0.0115,  0.0273,  ..., -0.0038, -0.0253, -0.0239],\n",
      "         [-0.0139,  0.0752,  0.0123,  ...,  0.0013, -0.0026, -0.0114]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<CatBackward0>)\n",
      "At module Embedding(256000, 3584, padding_idx=0)\n",
      "With module input (tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),)\n",
      "=== Model Output ===\n",
      "[Sample 0] LetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLetLet\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def inject_prompt_embeddings_via_forward_hook(model, prompt_embeds: torch.Tensor):\n",
    "    \"\"\"\n",
    "    A context manager to override the *output* of the embedding layer\n",
    "    for the multi-token \"prompt\" pass. For single-token steps,\n",
    "    we do nothing (use normal embeddings).\n",
    "    \"\"\"\n",
    "    def embedding_output_hook(module, module_input, module_output):\n",
    "        \"\"\"\n",
    "        module_output is the float embeddings of shape [batch_size, seq_len, hidden_dim].\n",
    "        We'll check if seq_len > 1 => prompt pass. Then we override.\n",
    "        \"\"\"\n",
    "        bsz, seq_len, hidden_dim = module_output.shape\n",
    "        if seq_len > 1:\n",
    "            # Overwrite with our custom prompt embeddings\n",
    "            #new_embeds = prompt_embeds[:, :seq_len, :]\n",
    "            print(f\"About to replace emebds shape {module_output.shape} with {prompt_embeds.shape}\")\n",
    "            print(f\"Specifically: {prompt_embeds}\")\n",
    "            print(f\"At module {module}\")\n",
    "            print(f\"With module input {module_input}\")\n",
    "            return prompt_embeds\n",
    "        else:\n",
    "            # Single-token step -> no changes\n",
    "            return module_output\n",
    "\n",
    "    # Register forward hook\n",
    "    handle = model.get_input_embeddings().register_forward_hook(embedding_output_hook)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "# Now let's test it\n",
    "\n",
    "batch_size, seq_len, hidden_dim = combo_embedding.shape\n",
    "dummy_input_ids = torch.full(\n",
    "    (batch_size, seq_len),\n",
    "    fill_value=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0,\n",
    "    dtype=torch.long,\n",
    "    device=combo_embedding.device\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "with inject_prompt_embeddings_via_forward_hook(model, combo_embedding):\n",
    "    generated_outputs = model.generate(\n",
    "        input_ids=dummy_input_ids,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "decoded = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "print(\"=== Model Output ===\")\n",
    "for i, text in enumerate(decoded):\n",
    "    print(f\"[Sample {i}] {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 256000])\n",
      "['<h1>\\n\\n\\n\\n\\n\\n do make a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBaking']\n"
     ]
    }
   ],
   "source": [
    "outputs1 = model.forward(inputs_embeds=combo_embedding)\n",
    "print(outputs1.logits.shape)\n",
    "print(tokenizer.batch_decode(torch.argmax(outputs1.logits, dim=-1), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /workspace/gemma_2_9b_instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found chat template intro length 4, outro length 5\n",
      "Loaded model with left-padding token: <pad>\n"
     ]
    }
   ],
   "source": [
    "from oat_evaluation.llms.autollm import AutoLLM\n",
    "\n",
    "llm = AutoLLM(\"/workspace/gemma_2_9b_instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,    106,   1645,    108,   2299,    577,  44528, 235336,    107,\n",
      "            108,    106,   2516,    108],\n",
      "        [     0,      0,      0,      2,    106,   1645,    108,  26620,    107,\n",
      "            108,    106,   2516,    108]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    [{\"role\": \"user\", \"content\": prompt}]\n",
    "    for prompt in [\"How to bake?\", \"LOL\"]\n",
    "]\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(model.device)\n",
    "print(tokenized_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: CausalLMOutputWithPast(loss=None, logits=tensor([[[-26.0938,  17.2500,  21.2344,  ..., -10.3750,  -3.5645, -13.8125],\n",
      "         [-24.4531,   4.1953, -16.5312,  ..., -13.5703, -10.8516, -25.2812],\n",
      "         [-25.4375,   2.9688, -17.9219,  ..., -14.4766, -11.4844, -26.1250],\n",
      "         ...,\n",
      "         [-28.8594,  -0.9131, -22.4062,  ..., -18.6250, -15.1875, -28.7188],\n",
      "         [-19.0000,  -0.9028, -11.5625,  ...,  -9.2812,  -8.0312, -18.0469],\n",
      "         [-15.5312,   1.6055,  -2.6191,  ...,  -3.9766,  -3.6816, -13.1875]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<MulBackward0>), past_key_values=<transformers.cache_utils.HybridCache object at 0x785e8892fd00>, hidden_states=None, attentions=None)\n",
      "Error trying to extract start length: 'CausalLMOutputWithPast' object has no attribute 'sequences'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CausalLMOutputWithPast' object has no attribute 'sequences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/workspace/GIT_SHENANIGANS/oat-2025/oat_evaluation/llms/autollm.py:269\u001b[0m, in \u001b[0;36mAutoLLM.generate_responses_forced\u001b[0;34m(self, prompts_or_embeddings, target_responses_or_embeddings, exposed_activations_request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     start_length \u001b[38;5;241m=\u001b[39m tokenized_chat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    267\u001b[0m     decoded_responses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mdecode(seq[start_length:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 269\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences\u001b[49m\n\u001b[1;32m    270\u001b[0m     ]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CausalLMOutputWithPast' object has no attribute 'sequences'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow to bake?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst, mix the ingredients.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m resp1 \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_responses_forced\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(resp1\u001b[38;5;241m.\u001b[39mresponses_strings)\n\u001b[1;32m      6\u001b[0m prompt_embeds \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mstring_to_embedding(prompt)\n",
      "File \u001b[0;32m/workspace/GIT_SHENANIGANS/oat-2025/oat_evaluation/llms/autollm.py:274\u001b[0m, in \u001b[0;36mAutoLLM.generate_responses_forced\u001b[0;34m(self, prompts_or_embeddings, target_responses_or_embeddings, exposed_activations_request)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError trying to extract start length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m     decoded_responses \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences\u001b[49m, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    275\u001b[0m     ]\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoded responses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoded_responses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResponses(\n\u001b[1;32m    280\u001b[0m     responses_strings\u001b[38;5;241m=\u001b[39mdecoded_responses,\n\u001b[1;32m    281\u001b[0m     responses_logits\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mscores,\n\u001b[1;32m    282\u001b[0m     activation_layers\u001b[38;5;241m=\u001b[39mactivations_list \u001b[38;5;28;01mif\u001b[39;00m exposed_activations_request \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    283\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CausalLMOutputWithPast' object has no attribute 'sequences'"
     ]
    }
   ],
   "source": [
    "prompt = \"How to bake?\"\n",
    "response = \"First, mix the ingredients.\"\n",
    "resp1 = llm.generate_responses_forced([prompt], [response])\n",
    "print(resp1.responses_strings)\n",
    "\n",
    "prompt_embeds = llm.string_to_embedding(prompt)\n",
    "resp_embeds = llm.string_to_embedding(response)\n",
    "resp2 = llm.generate_responses_forced([prompt], [response])\n",
    "print(resp2.responses_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_autoregressive_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    initial_input_ids: torch.Tensor,\n",
    "    max_new_tokens=50,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    custom_embedding_fn=None,\n",
    "    device=\"cuda\",\n",
    "    eos_token_id=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom autoregressive loop. Feeds the entire sequence to model.forward each step,\n",
    "    picks the next token from the logits, appends to sequence, and repeats.\n",
    "    \n",
    "    Args:\n",
    "      model: A HuggingFace CausalLM model or similar\n",
    "      tokenizer: A HuggingFace tokenizer\n",
    "      initial_input_ids: shape (batch_size, seq_len); your starting tokens\n",
    "      max_new_tokens: how many tokens to generate\n",
    "      temperature, top_p: sampling parameters\n",
    "      custom_embedding_fn: optional function that overrides token+pos embeddings\n",
    "      device: \"cuda\" or \"cpu\"\n",
    "      eos_token_id: If not None, generation will stop upon generating this token\n",
    "\n",
    "    Returns:\n",
    "      A list of token IDs (per batch) including the newly generated tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move the current tokens to device\n",
    "    input_ids = initial_input_ids.to(device)\n",
    "    batch_size = input_ids.size(0)\n",
    "\n",
    "    # We'll store outputs in a Python list so we can keep appending\n",
    "    # (though you can also keep it in a single tensor if you prefer).\n",
    "    generated = [input_ids for _ in range(batch_size)]\n",
    "    # Actually, to keep it simpler for multi-batch, let's keep it as one tensor:\n",
    "    generated = input_ids.clone()\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        seq_len = generated.size(1)\n",
    "\n",
    "        if custom_embedding_fn is None:\n",
    "            # Normal forward pass: pass input_ids\n",
    "            outputs = model.forward(input_ids=generated)\n",
    "            logits = outputs.logits  # shape [batch_size, seq_len, vocab_size]\n",
    "        else:\n",
    "            # If you have a custom function that transforms your input_ids into embeddings\n",
    "            # and you do not want the model to do the token-embedding lookup,\n",
    "            # you can pass `inputs_embeds` directly:\n",
    "            with torch.no_grad():\n",
    "                # custom_embedding_fn should produce a float tensor of shape [batch_size, seq_len, hidden_dim]\n",
    "                # possibly by hooking or by direct construction\n",
    "                inputs_embeds = custom_embedding_fn(model, generated)\n",
    "                outputs = model.forward(inputs_embeds=inputs_embeds)\n",
    "                logits = outputs.logits\n",
    "\n",
    "        # Take the last-token's logits (shape [batch_size, vocab_size])\n",
    "        next_logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply temperature\n",
    "        if temperature != 1.0:\n",
    "            next_logits = next_logits / temperature\n",
    "\n",
    "        # Apply top-p (nucleus) sampling\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)\n",
    "            cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
    "\n",
    "            # Remove tokens with cumulative probability > top_p\n",
    "            cutoff = (cumulative_probs > top_p).float().argmax(dim=-1)\n",
    "            # We'll build a mask to zero out probabilities beyond the cutoff\n",
    "            # For each batch item, find the cutoff index\n",
    "            for b in range(batch_size):\n",
    "                # index up to cutoff[b] inclusive\n",
    "                sorted_logits[b, cutoff[b]+1 :] = float(\"-inf\")\n",
    "\n",
    "            # Re-sort back to original positions\n",
    "            _, original_indices = torch.sort(sorted_indices, descending=False)\n",
    "            next_logits = sorted_logits.gather(1, original_indices)\n",
    "\n",
    "        # Convert logits -> probabilities\n",
    "        probs = F.softmax(next_logits, dim=-1)\n",
    "\n",
    "        # Sample from the distribution\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1)  # [batch_size, 1]\n",
    "\n",
    "        # Append new token to `generated`\n",
    "        generated = torch.cat([generated, next_tokens], dim=1)  # shape [batch_size, seq_len+1]\n",
    "\n",
    "        # If we have an EOS token, we can stop if all sequences ended\n",
    "        if eos_token_id is not None:\n",
    "            # Check if any of the new tokens are EOS\n",
    "            is_eos = (next_tokens == eos_token_id).all()\n",
    "            if is_eos:\n",
    "                break\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "### Example usage\n",
    "\n",
    "# Suppose you have a model, tokenizer, and an initial text prompt:\n",
    "# model = ...\n",
    "# tokenizer = ...\n",
    "prompt_text = \"The quick brown fox\"\n",
    "initial_inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# No custom embedding override (pure normal next-token generation):\n",
    "generated_tokens = custom_autoregressive_generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    initial_input_ids=initial_inputs[\"input_ids\"],\n",
    "    max_new_tokens=30,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "decoded = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print(\"Decoded text:\", decoded[0])  # single batch, index 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
